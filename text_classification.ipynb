{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90d59a88-7f8e-4c8e-a4a4-7d95c7902bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in f:\\conda\\lib\\site-packages (0.23.4)\n",
      "Collecting huggingface_hub\n",
      "  Obtaining dependency information for huggingface_hub from https://files.pythonhosted.org/packages/0b/05/31b21998f68c31e7ffcc27ff08531fb9af5506d765ce8d661fb0036e6918/huggingface_hub-0.24.5-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in f:\\conda\\lib\\site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in f:\\conda\\lib\\site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in f:\\conda\\lib\\site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in f:\\conda\\lib\\site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: requests in f:\\conda\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in f:\\conda\\lib\\site-packages (from huggingface_hub) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in f:\\conda\\lib\\site-packages (from huggingface_hub) (4.7.1)\n",
      "Requirement already satisfied: colorama in f:\\conda\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\conda\\lib\\site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\conda\\lib\\site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\conda\\lib\\site-packages (from requests->huggingface_hub) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\conda\\lib\\site-packages (from requests->huggingface_hub) (2024.7.4)\n",
      "Using cached huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "Installing collected packages: huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.23.4\n",
      "    Uninstalling huggingface-hub-0.23.4:\n",
      "      Successfully uninstalled huggingface-hub-0.23.4\n",
      "Successfully installed huggingface_hub-0.24.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "exo 0.0.1 requires huggingface-hub==0.23.4, but you have huggingface-hub 0.24.5 which is incompatible.\n",
      "exo 0.0.1 requires numpy==2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "exo 0.0.1 requires protobuf==5.27.1, but you have protobuf 4.25.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99222989-2020-4bed-a31b-fed1d9690899",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a023f-f5a1-4d0b-a82c-e51eac185cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers==4.12.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c7962fc-b43b-49fd-87f5-b3b16e6942b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Данила\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import lightgbm as lgb\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11d8fbaf-8990-4e1f-b0b6-a0defa9e575e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A fantastic experience from start to finish.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Poor plot and terrible acting.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie was amazing! I loved it.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I hated this film. It was awful.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A fantastic experience from start to finish.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review  sentiment\n",
       "0  A fantastic experience from start to finish.          1\n",
       "1                Poor plot and terrible acting.          0\n",
       "2           This movie was amazing! I loved it.          1\n",
       "3              I hated this film. It was awful.          0\n",
       "4  A fantastic experience from start to finish.          1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_synthetic_reviews(num_reviews=1000):\n",
    "    positive_reviews = [\n",
    "        \"This movie was amazing! I loved it.\",\n",
    "        \"A great film with stunning visuals.\",\n",
    "        \"Excellent plot and great acting.\",\n",
    "        \"I thoroughly enjoyed this movie.\",\n",
    "        \"A fantastic experience from start to finish.\"\n",
    "    ]\n",
    "    \n",
    "    negative_reviews = [\n",
    "        \"This was a terrible movie.\",\n",
    "        \"I hated this film. It was awful.\",\n",
    "        \"Poor plot and terrible acting.\",\n",
    "        \"I did not enjoy this movie at all.\",\n",
    "        \"A waste of time and money.\"\n",
    "    ]\n",
    "    \n",
    "    reviews = []\n",
    "    sentiments = []\n",
    "    \n",
    "    for _ in range(num_reviews // 2):\n",
    "        reviews.append(random.choice(positive_reviews))\n",
    "        sentiments.append(1)\n",
    "        reviews.append(random.choice(negative_reviews))\n",
    "        sentiments.append(0)\n",
    "    \n",
    "    return pd.DataFrame({'review': reviews, 'sentiment': sentiments})\n",
    "\n",
    "synthetic_data = generate_synthetic_reviews()\n",
    "synthetic_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36495dd3-6ae5-4d92-81b2-7d63575af70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a35c839-5e23-49e0-b427-5d4853612df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Данила\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Преобразование текста в нижний регистр и удаление стоп-слов\n",
    "    text = text.lower()\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "data['review'] = data['review'].apply(preprocess_text)\n",
    "data['sentiment'] = data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27194d7a-d706-4ac9-843a-49a761b5a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['review']\n",
    "y = data['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf154b6-7950-4a8e-9783-5e178b6d3d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "F:\\Conda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len=128):\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "\n",
    "    for text in texts:\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True\n",
    "        )\n",
    "        all_input_ids.append(inputs['input_ids'])\n",
    "        all_attention_masks.append(inputs['attention_mask'])\n",
    "\n",
    "    return np.array(all_input_ids), np.array(all_attention_masks)\n",
    "\n",
    "X_train_ids, X_train_masks = bert_encode(X_train, tokenizer)\n",
    "X_test_ids, X_test_masks = bert_encode(X_test, tokenizer)\n",
    "\n",
    "def create_bert_embedding(ids, masks):\n",
    "    bert_outputs = bert_model([ids, masks])\n",
    "    return bert_outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "X_train_bert = create_bert_embedding(X_train_ids, X_train_masks)\n",
    "X_test_bert = create_bert_embedding(X_test_ids, X_test_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5ec9c00-890f-4c36-9f08-624ec673259f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.12.3\n",
      "  Obtaining dependency information for transformers==4.12.3 from https://files.pythonhosted.org/packages/2b/db/9145468493530a60046e59fc2a72b974591e38c1f3fc1b6c75112a91e492/transformers-4.12.3-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.12.3-py3-none-any.whl.metadata (56 kB)\n",
      "     ---------------------------------------- 0.0/56.6 kB ? eta -:--:--\n",
      "     ------------- ------------------------ 20.5/56.6 kB 330.3 kB/s eta 0:00:01\n",
      "     -------------------- ----------------- 30.7/56.6 kB 330.3 kB/s eta 0:00:01\n",
      "     ---------------------------------- --- 51.2/56.6 kB 327.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 56.6/56.6 kB 269.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in f:\\conda\\lib\\site-packages (from transformers==4.12.3) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in f:\\conda\\lib\\site-packages (from transformers==4.12.3) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in f:\\conda\\lib\\site-packages (from transformers==4.12.3) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in f:\\conda\\lib\\site-packages (from transformers==4.12.3) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in f:\\conda\\lib\\site-packages (from transformers==4.12.3) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in f:\\conda\\lib\\site-packages (from transformers==4.12.3) (2022.7.9)\n",
      "Requirement already satisfied: requests in f:\\conda\\lib\\site-packages (from transformers==4.12.3) (2.32.3)\n",
      "Collecting sacremoses (from transformers==4.12.3)\n",
      "  Obtaining dependency information for sacremoses from https://files.pythonhosted.org/packages/0b/f0/89ee2bc9da434bd78464f288fdb346bc2932f2ee80a90b2a4bbbac262c74/sacremoses-0.1.1-py3-none-any.whl.metadata\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting tokenizers<0.11,>=0.10.1 (from transformers==4.12.3)\n",
      "  Downloading tokenizers-0.10.3.tar.gz (212 kB)\n",
      "     ---------------------------------------- 0.0/212.7 kB ? eta -:--:--\n",
      "     ------- -------------------------------- 41.0/212.7 kB ? eta -:--:--\n",
      "     --------------------- ---------------- 122.9/212.7 kB 1.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 212.7/212.7 kB 2.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.27 in f:\\conda\\lib\\site-packages (from transformers==4.12.3) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in f:\\conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.3) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in f:\\conda\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.3) (4.7.1)\n",
      "Requirement already satisfied: colorama in f:\\conda\\lib\\site-packages (from tqdm>=4.27->transformers==4.12.3) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\conda\\lib\\site-packages (from requests->transformers==4.12.3) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\conda\\lib\\site-packages (from requests->transformers==4.12.3) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\conda\\lib\\site-packages (from requests->transformers==4.12.3) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\conda\\lib\\site-packages (from requests->transformers==4.12.3) (2024.7.4)\n",
      "Requirement already satisfied: click in f:\\conda\\lib\\site-packages (from sacremoses->transformers==4.12.3) (8.1.7)\n",
      "Requirement already satisfied: joblib in f:\\conda\\lib\\site-packages (from sacremoses->transformers==4.12.3) (1.2.0)\n",
      "Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n",
      "   ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.1 MB 5.4 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.5/3.1 MB 7.2 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.5/3.1 MB 7.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.0/3.1 MB 7.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.4/3.1 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.9/3.1 MB 6.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.1/3.1 MB 7.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.3/3.1 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.7/3.1 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.1/3.1 MB 6.6 MB/s eta 0:00:00\n",
      "Using cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build tokenizers\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [51 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "  copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\visualizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "  copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\tools\\visualizer-styles.css -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4368ea3a-7f75-4a46-8ad6-500f25b544f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model = lgb.LGBMClassifier()\n",
    "lgbm_model.fit(X_train_bert, y_train)\n",
    "y_pred_lgbm = lgbm_model.predict(X_test_bert)\n",
    "\n",
    "print(f\"LightGBM Accuracy: {accuracy_score(y_test, y_pred_lgbm)}\")\n",
    "print(f\"LightGBM Precision: {precision_score(y_test, y_pred_lgbm)}\")\n",
    "print(f\"LightGBM Recall: {recall_score(y_test, y_pred_lgbm)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f14731-9657-4819-a2f7-80d5ab24b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Распределение отзывов\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data['sentiment'])\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Примеры отзывов\n",
    "print(data['review'].head())\n",
    "print(data['sentiment'].head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
